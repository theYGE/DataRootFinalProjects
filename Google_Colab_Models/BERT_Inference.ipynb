{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Inference.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qSlIjM2Py4V",
        "colab_type": "text"
      },
      "source": [
        "# Ukrainian Stories For Kids Generation Based On Multilingual Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_acgI-kQIRC",
        "colab_type": "text"
      },
      "source": [
        "The goal of this final project was to train multilingual Bert from Google on Ukrainian corpus to compare original model with trained version on Masked Language Model and Next Sentence Prediction combined to see how good the original model was and if some improvement could have been made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8s8Sk2sQ2s6",
        "colab_type": "text"
      },
      "source": [
        "One of the biggest challenges that was faced in this project was to find a suitable dataset. Since Ukrainian corpuses are not widespread it was necessary to create one. The initial guess was that although BERT is claiming to be multilingual, it was not performing well on low-resource languages like Ukrainian. The assumption proved itself to be true as you will be able to see later. Short stories for kids and fairytales are a good candidates for training corpus since they are comprised of not so big of a voabulary and generally have similar narration structure. It's important to mention that vocabulary of a child is not as developed as that of an adult, so the model might do a much better job training on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHINw_DLAQRp",
        "colab_type": "code",
        "outputId": "83805b7b-893b-4977-e860-edf763741536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "source": [
        "!pip install pytorch-nlp\n",
        "!pip install tokenize_uk\n",
        "!pip install pytorch_transformers"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.16.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas->pytorch-nlp) (1.12.0)\n",
            "Requirement already satisfied: tokenize_uk in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tokenize_uk) (1.12.0)\n",
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.6.8)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.9.205)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.16.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.1.82)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.205 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.12.205)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.205->boto3->pytorch_transformers) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.15,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.205->boto3->pytorch_transformers) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.205->boto3->pytorch_transformers) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJwPIPmNARXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from pytorch_transformers import *\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import tokenize_uk\n",
        "import os\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO_q0kpCdmh0",
        "colab_type": "code",
        "outputId": "fde38903-7bc9-44e0-accb-56f9497b184d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LQBcqqQAKbm",
        "colab_type": "code",
        "outputId": "fc2d4da7-280e-4c9a-90d3-d0cfe395af1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(\"Found GPU at: {}\".format(device_name))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA3GISGwbUbt",
        "colab_type": "code",
        "outputId": "9d1426f6-afda-4b70-d036-1d03d282a904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaD4xjWCAUQ3",
        "colab_type": "code",
        "outputId": "9bce8885-a292-4fbb-e118-3309342205d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "device"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_za-s4KRlFA",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Alteration based on Trained Masked Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL0XrzhLT2Ct",
        "colab_type": "text"
      },
      "source": [
        "The first part of the project focuses on altering stories based on Masked Language Model. The input for this part is comprised of three stories for kids. Each story is broken down into sentences and in each sentence one word is picked by random and is masked by [MASK] token. It's importnat to note that in reality we might have a compound word that, if masked, would be masked by multiple [MASK] tokens, but for the sake of the project each word, regardless of the size, was masked by one [MASK] token. The idea is to see what the original BERT might plaace in place of the masked token and compare the results to the trained version to see which one understands Ukrainian language better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iVA22PavLr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import *\n",
        "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
        "from random import random, randrange, randint, shuffle, choice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N8E549_di4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Masks one word at random by a single [MASK] token in a given sentence\n",
        "def mask_token_in_sentence(sentence):\n",
        "  tokenized_sentence = tokenize_uk.tokenize_words(sentence)\n",
        "  number_of_tokens = len(tokenized_sentence)\n",
        "  # print(tokenized_sentence, number_of_tokens)\n",
        "  masked_token = False\n",
        "  while (not masked_token):\n",
        "    # print(number_of_tokens)\n",
        "    index = randrange(number_of_tokens)\n",
        "    # print(index)\n",
        "    # To eliminate tokenization of punctioation like .,;:\n",
        "    delims = {\", \", \".\", \"!\", \":\", \"?\", \"'\", \";\", ''}\n",
        "    if (not tokenized_sentence[index] in delims):\n",
        "      tokenized_sentence[index] = \"[MASK]\"\n",
        "      masked_token = True\n",
        "  # print(tokenized_sentence)\n",
        "  reconstructed_sentence = \" \".join(tokenized_sentence)\n",
        "  return reconstructed_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNDdE4dWKSUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns a set of unique punctuation marks in a given texts\n",
        "def get_unique_delimiters(texts):\n",
        "  delimiters = set()\n",
        "  for text in texts:\n",
        "    for word in tokenize_uk.tokenize_words(text):\n",
        "      if (len(word) == 1 and not word in delimiters and not word.isalpha() and not word.isdigit()):\n",
        "        delimiters.add(word)\n",
        "  return delimiters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubCWnWeVDLK",
        "colab_type": "text"
      },
      "source": [
        "Please Note: you need to have a corresponding folder with fairytales on your google drive to load the data. Refer to the GitHub repositrory to download it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoZO5G1vEicO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_text = \"\"\n",
        "with open(\"/content/drive/My Drive/Fairytales/test/Тхір і лис.txt\", 'r') as f:\n",
        "  first_text = f.read()\n",
        "\n",
        "second_text = \"\"\n",
        "with open (\"/content/drive/My Drive/Fairytales/test/Чудесні бички.txt\", 'r') as f:\n",
        "  second_text = f.read()\n",
        "  \n",
        "third_text = \"\"\n",
        "with open (\"/content/drive/My Drive/Fairytales/test/Їжачок та дівчинка.txt\", \"r\") as f:\n",
        "  third_text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDlcsSjwInIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# texts = [first_text, second_text, third_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By7UrDsCIysd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unique_delimiters = get_unique_delimiters(texts)\n",
        "# print(unique_delimiters)\n",
        "# print(len(unique_delimiters))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL7uejDcI-mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleans texts by substituting similar punctuation marks with a single standard\n",
        "def clean_texts(texts):\n",
        "\n",
        "  dashes = {'–', '—', '―', '~'} # replace with -\n",
        "  special_symbols = {'№', '_', '<', '>', '|', ']', '*', '[', '^', '&'} # replace with \"\"\n",
        "  apostrophes = {'’', '‘'} # replace with '\n",
        "  direct_speech = {'“', '»', '«'} # replace with '\"'\n",
        "  three_dots = {'…'} # replace with '.\n",
        "\n",
        "  counter = 0\n",
        "  for i in range(len(texts)):\n",
        "    print(\"Processing text: \", i)\n",
        "    text = texts[i]\n",
        "    words = []\n",
        "    tokenized_words = tokenize_uk.tokenize_words(text)\n",
        "    for word in tokenized_words:\n",
        "      added = False\n",
        "\n",
        "      for dash in dashes:\n",
        "        if (dash in word):\n",
        "          new_word = word.replace(dash, \"-\")\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "\n",
        "      for special_symbol in special_symbols:\n",
        "        if(special_symbol in word):\n",
        "          new_word = word.replace(special_symbol, \"\")\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "      \n",
        "      for apostrophe in apostrophes:\n",
        "        if (apostrophe in word):\n",
        "          new_word = word.replace(apostrophe, \"'\")\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "\n",
        "      for direct in direct_speech:\n",
        "        if (direct in word):\n",
        "          new_word = word.replace(direct, '\"')\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "      \n",
        "      for dots in three_dots:\n",
        "        if (dots in word):\n",
        "          counter += 1\n",
        "          new_word = word.replace(dots, '.')\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "      if (not added):\n",
        "        words.append(word)\n",
        "    reconstructed_text = \" \".join(words)\n",
        "    texts[i] = reconstructed_text \n",
        "  return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUVKpBF8Q94A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# texts = clean_texts(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPiA4fRJKqzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprares text for sentence tokenization by tokenize_uk\n",
        "def prepare_for_tokenization(texts):\n",
        "  new_texts = []\n",
        "  for i in range(len(texts)):\n",
        "    text = texts[i]\n",
        "    text = text.replace(\"?\", \"?.\")\n",
        "    text = text.replace(\"!\", \"!.\")\n",
        "    text = text.replace(\":\", \":.\")\n",
        "    text = text.replace(\". -\", \". \")\n",
        "    new_texts.append(text)\n",
        "  return new_texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSNTDCssJGaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_texts = prepare_for_tokenization(texts)\n",
        "# print(\"Number of texts: \", len(new_texts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CTxAhaSK6LQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizes texts to sentences\n",
        "def tokenize_texts_to_sentences(texts):\n",
        "  text_sentences = []\n",
        "  for text in texts:\n",
        "    text_to_add = []\n",
        "    text_to_add += tokenize_uk.tokenize_sents(text)\n",
        "    text_sentences.append(text_to_add)\n",
        "  return text_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSVbcM8dJKI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_sentences = tokenize_texts_to_sentences(new_texts)\n",
        "# print(\"Number of tokenized texts: \", len(text_sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi6cC6ANJoGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(len(text_sentences)):\n",
        "#   print(i, \" contains \", len(text_sentences[i]), \" sentences\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpc2rnafKsYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(new_texts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iecDVgVILhRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleans sentences after tokenization\n",
        "def clean_after_tokenization(text_sentences):\n",
        "  cleaned = []\n",
        "\n",
        "  for j in range(len(text_sentences)):\n",
        "    text_to_work = text_sentences[j]\n",
        "    for i in range(len(text_to_work)):\n",
        "      sentence = text_to_work[i]\n",
        "      sentence = sentence.replace(\"?.\", \"?\")\n",
        "      sentence = sentence.replace(\"!.\", \"!\")\n",
        "      sentence = sentence.replace(\":.\", \":\")\n",
        "      text_to_work[i] = sentence\n",
        "    cleaned.append(text_to_work)\n",
        "  return cleaned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6qbrhkqJOV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_sentences = clean_after_tokenization(text_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1TFh47gLHcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(len(text_sentences)):\n",
        "#   print(i, \" contains \", len(text_sentences[i]), \" sentences\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-kbtzSSNDLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Masks words in sentences\n",
        "def mask_tokens(text_sentences):\n",
        "  masked = []\n",
        "  for j in range(len(text_sentences)):\n",
        "    text_to_work = text_sentences[j]\n",
        "    for i in range(len(text_to_work)):\n",
        "      sentence = text_to_work[i]\n",
        "      # print(sentence)\n",
        "      sentence = mask_token_in_sentence(sentence)\n",
        "      text_to_work[i] = sentence\n",
        "    masked.append(text_to_work)\n",
        "  return masked"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYx36SfNFdhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_sentences = mask_tokens(text_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3f4f0hjMm1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(len(text_sentences)):\n",
        "#   print(i, \" contains \", len(text_sentences[i]), \" sentences\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_UG5ZM_NocX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"Number of texts: \", len(text_sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2UEie8xMoIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(len(text_sentences[0])):\n",
        "#   sentence = text_sentences[0][i]\n",
        "#   sentence = sentence.replace(\"[CLS] \", \"\")\n",
        "#   sentence = sentence.replace(\"[SEP] \", \"\")\n",
        "#   text_sentences[0][i] = sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4crDPLGzBBtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for sent in text_sentences[0]:\n",
        "#   print(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OXBU2LQgyA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading tokenizer from traineed BERT\n",
        "tokenizer_path = \"/content/drive/My Drive/BERT/finetuned_for_inference/vocab.txt\"\n",
        "trained_tokenizer = BertTokenizer(tokenizer_path, do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6EZCrewpmU",
        "colab_type": "code",
        "outputId": "114d9a2a-73e8-456f-ff16-eaa7afe5b34e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loading tokenizer from original BERT\n",
        "basic_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 995526/995526 [00:00<00:00, 1993057.27B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4PXv4T3QvZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_sentences[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIIaUvb6SVsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae5321e2-caf0-4065-b32a-c9692b6ecedc"
      },
      "source": [
        "# Loads trained BertFroPreTraining model\n",
        "model_state_dict_path = \"/content/drive/My Drive/BERT/finetuned_for_inference/pytorch_model.bin\"\n",
        "model_folder_path = \"/content/drive/My Drive/BERT/finetuned_for_inference\"\n",
        "\n",
        "model_state_dict = torch.load(model_state_dict_path,  map_location='cpu')\n",
        "trained_model = BertForPreTraining.from_pretrained(model_folder_path, state_dict=model_state_dict)\n",
        "trained_model.eval()\n",
        "trained_model.to(device)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForPreTraining(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): BertPreTrainingHeads(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=119547, bias=False)\n",
              "    )\n",
              "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTGRn5dHwmoB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33f2ec53-d510-4ce6-f324-f33043a5168c"
      },
      "source": [
        "# Loads basic BertForPreTraining model\n",
        "basic_model = BertForPreTraining.from_pretrained('bert-base-multilingual-cased')\n",
        "basic_model.eval()\n",
        "basic_model.to(device)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 521/521 [00:00<00:00, 104446.63B/s]\n",
            "100%|██████████| 714314041/714314041 [00:25<00:00, 28199757.21B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForPreTraining(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): BertPreTrainingHeads(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=119547, bias=False)\n",
              "    )\n",
              "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_feuu7WVv3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trained_model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1I5sQELTGcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns segment ids and indexed tokens for a given text\n",
        "def get_segments_and_tokens(text_input, given_tokenizer):\n",
        "  text = text_input\n",
        "  tokenized_text = []\n",
        "  indexed_tokens = []\n",
        "  segment_ids = []\n",
        "\n",
        "  print(\"Number of sentences in text: \", len(text))\n",
        "  longest_sentence = 0\n",
        "  for sent in text:\n",
        "    if (len(sent) > longest_sentence):\n",
        "      longest_sentence = len(sent)\n",
        "  print(\"Longest sentence: \", longest_sentence)\n",
        "\n",
        "  for i in range(len(text)):\n",
        "    text[i] = \"[CLS] \" + text[i] + \" [SEP]\" \n",
        "\n",
        "  for i in range(len(text)):\n",
        "    tokenized_text.append(given_tokenizer.tokenize(text[i]))\n",
        "    # print(tokenized_text[i])\n",
        "    indexed_tokens.append(given_tokenizer.convert_tokens_to_ids(tokenized_text[i]))\n",
        "\n",
        "  indexed_tokens = pad_sequences(indexed_tokens, maxlen=longest_sentence, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  print(\"First indexed tokens length: \", len(indexed_tokens[0]))\n",
        "\n",
        "  for i in range(len(text)):\n",
        "    segment = []\n",
        "    num_tokens = len(indexed_tokens[i])\n",
        "    segment_ids.append([0] * num_tokens)\n",
        "\n",
        "  print(\"Segment ids length: \", len(segment_ids))\n",
        "  print(\"First segment ids length: \", len(segment_ids[0]))\n",
        "\n",
        "  return segment_ids, indexed_tokens\n",
        "\n",
        "  # segments_tensor = (torch.tensor(segment_ids))\n",
        "  # tokens_tensor = torch.tensor(indexed_tokens)\n",
        "\n",
        "  # return segments_tensor, tokens_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLg3dyX1U8FJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Changes [MASK] tokens from predictions by choosing word with highest probability\n",
        "def make_inference(predictions, inference):\n",
        "  # predictions = masked_lm_logits_scores\n",
        "\n",
        "  # inference = indexed_tokens\n",
        "  new_inference = []\n",
        "  for i in range(len(inference)):\n",
        "    # print(i)\n",
        "    tokens = inference[i]\n",
        "    # print(tokens)\n",
        "    for j in range(len(tokens)):\n",
        "      if (tokens[j] == 103):\n",
        "        # print(103)\n",
        "        token_to_insert = torch.argmax(predictions[i][j]).item()\n",
        "        tokens[j] = token_to_insert\n",
        "    # inference[i] = tokens\n",
        "    new_inference.append(tokens)\n",
        "  return new_inference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUFAXE-TVkNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convertes inferred tokens to text and cleans it\n",
        "def convert_inference_to_sentences(inference, given_tokenizer):\n",
        "  predicted_sentences = []\n",
        "  for x in range(len(inference)):\n",
        "    first_sentence = given_tokenizer.convert_ids_to_tokens(inference[x])\n",
        "    first_sentence = given_tokenizer.convert_tokens_to_string(first_sentence)\n",
        "    first_sequence = first_sentence.replace(\"[PAD]\", \"\")\n",
        "    first_sequence = first_sequence.replace(\"[CLS] \", \"\")\n",
        "    first_sequence = first_sequence.replace(\" [SEP]\", \"\")\n",
        "    predicted_sentences.append(first_sequence)\n",
        "  return predicted_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7TcxIbfuxDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generates Masked Language Model Result for a given text with given tokenizer and given model\n",
        "def generate_results(text_sentences, given_model, given_tokenizer):\n",
        "    predicted_texts = []\n",
        "\n",
        "    for i in range(len(text_sentences)):\n",
        "        print(\"Processing text: \", i)\n",
        "        text = text_sentences[i]\n",
        "        segment_ids, indexed_tokens = get_segments_and_tokens(text, given_tokenizer)\n",
        "        print(\"Recieved segments and indexed_tokens\")\n",
        "\n",
        "        segments_tensor = (torch.tensor(segment_ids))\n",
        "        tokens_tensor = torch.tensor(indexed_tokens)\n",
        "\n",
        "        print(\"Model started\")\n",
        "\n",
        "        input_length = len(segments_tensor)\n",
        "        print(input_length)\n",
        "\n",
        "        logits = []\n",
        "        text = []\n",
        "        for j in range(0, input_length, 10):\n",
        "\n",
        "            left = j\n",
        "            right = j + 10\n",
        "            if (right > input_length):\n",
        "                right = input_length\n",
        "\n",
        "            if (left == input_length - 1):\n",
        "                right = input_length\n",
        "\n",
        "            segments_portion = segment_ids[left:right]\n",
        "            tokens_portion = indexed_tokens[left:right]\n",
        "            segments_tensor = (torch.tensor(segments_portion))\n",
        "            tokens_tensor = torch.tensor(tokens_portion)\n",
        "\n",
        "            tokens_tensor = tokens_tensor.to(device)\n",
        "            segments_tensor = segments_tensor.to(device)\n",
        "\n",
        "            print(\"Processing from {} to {}\".format(left, right))\n",
        "            with torch.no_grad():\n",
        "                masked_lm_logits_scores, _ = given_model(tokens_tensor, segments_tensor)\n",
        "                # logits.append(masked_lm_logits_scores)\n",
        "\n",
        "            # logits = np.stack(logits, axis = 0)\n",
        "            print(\"Model finished\")\n",
        "\n",
        "            predictions = masked_lm_logits_scores\n",
        "            inference = indexed_tokens[left:right]\n",
        "\n",
        "            new_inference = make_inference(predictions, inference)\n",
        "            print(\"Recieved new inference\")\n",
        "\n",
        "            predicted_sentences = convert_inference_to_sentences(new_inference, given_tokenizer)\n",
        "            print(\"Recieved predicted_sentences\")\n",
        "\n",
        "            text += predicted_sentences\n",
        "\n",
        "        predicted_texts.append(text)\n",
        "        print(\"Appended new sentences for: \", i)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    saved_predicted = predicted_texts\n",
        "\n",
        "    cleaned = []\n",
        "    for j in range(len(saved_predicted)):\n",
        "        text_to_work = saved_predicted[j]\n",
        "        for i in range(len(text_to_work)):\n",
        "            sentence = text_to_work[i]\n",
        "            sentence = sentence.replace(\" ?\", \"?\")\n",
        "            sentence = sentence.replace(\" !\", \"!\")\n",
        "            sentence = sentence.replace(\" :\", \":\")\n",
        "            sentence = sentence.replace(\"  \", \"\")\n",
        "            sentence = sentence.replace(\" ,\", \",\")\n",
        "            sentence = sentence.replace(\" .\", \".\")\n",
        "            text_to_work[i] = sentence\n",
        "        cleaned.append(text_to_work)\n",
        "\n",
        "    cleaned_texts = cleaned\n",
        "    # cleaned_texts = []\n",
        "    # for clean in cleaned:\n",
        "    #     cleaned_texts.append(\" \".join(clean))\n",
        "\n",
        "    return cleaned_texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCe1P9Ucu2ZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    texts = [first_text, second_text, third_text]\n",
        "\n",
        "    unique_delimiters = get_unique_delimiters(texts)\n",
        "    print(unique_delimiters)\n",
        "    print(len(unique_delimiters))\n",
        "\n",
        "    texts = clean_texts(texts)\n",
        "\n",
        "    new_texts = prepare_for_tokenization(texts)\n",
        "    print(\"Number of texts: \", len(new_texts))\n",
        "\n",
        "    text_sentences = tokenize_texts_to_sentences(new_texts)\n",
        "    print(\"Number of tokenized texts: \", len(text_sentences))\n",
        "\n",
        "    for i in range(len(text_sentences)):\n",
        "        print(i, \" contains \", len(text_sentences[i]), \" sentences\")\n",
        "\n",
        "    print(new_texts[0])\n",
        "\n",
        "    text_sentences = clean_after_tokenization(text_sentences)\n",
        "\n",
        "    for i in range(len(text_sentences)):\n",
        "        print(i, \" contains \", len(text_sentences[i]), \" sentences\")\n",
        "\n",
        "    text_sentences = mask_tokens(text_sentences)\n",
        "\n",
        "    for i in range(len(text_sentences)):\n",
        "        print(i, \" contains \", len(text_sentences[i]), \" sentences\")\n",
        "\n",
        "    print(\"Number of texts: \", len(text_sentences))\n",
        "\n",
        "    for sent in text_sentences[0]:\n",
        "        print(sent)\n",
        "\n",
        "    cleaned_trained = generate_results(text_sentences, trained_model, trained_tokenizer)\n",
        "\n",
        "    for j in range(len(text_sentences)):\n",
        "        current = text_sentences[j]\n",
        "        for i in range(len(current)):\n",
        "          sentence = current[i]\n",
        "          sentence = sentence.replace(\"[CLS] \", \"\")\n",
        "          sentence = sentence.replace(\" [SEP]\", \"\")\n",
        "          current[i] = sentence\n",
        "        text_sentences[j] = current\n",
        "\n",
        "    cleaned_basic = generate_results(text_sentences, basic_model, basic_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkI3qqKJdnX2",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2UpwBYAdv0_",
        "colab_type": "text"
      },
      "source": [
        "Below are the sentences with masked words and predictions based on original and trained models. Again, it might be the case that the word would be masked by multiple tokens as in the original implementation, but for the sake of this project any word, regardless of size, was masked by exactly one [MASK] token to see how two models would perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9f3WYxGeO8y",
        "colab_type": "text"
      },
      "source": [
        "From the inference we can see some interesting results that need to be discussed. First of all, unfortunately, original BERT does a poor job on masked words. For example, in sentence \n",
        "\n",
        "*'[CLS] Раз прийшов лис до [MASK] в гості , та й тхір його гарно погостив . [SEP]'* \n",
        "\n",
        "original BERT outputs \n",
        "\n",
        "*'Раз прийшов лис доu в гості, та й тхір його гарно погостив.'* \n",
        "and in \n",
        "\n",
        "*'[CLS] Той чоловік пригонить бички додому та й [MASK] : [SEP]'* \n",
        "the output was \n",
        "\n",
        "*'Той чоловік пригонить бички додому та йо:'*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHpG7Tz3fnsB",
        "colab_type": "text"
      },
      "source": [
        "As we can see, original BERT not only fails to predict correct part of speech, but even fails to make predictions in ukrainian as we can see from the first sentence. This is a clear proof of the fact that the corpus that was used to train Multilingual Bert for Ukrainian didn't generalize language well enough. This can be attributed to the fact that Ukrainian is a low-resource language and it's hard to create a good corpus. It's also possible that due to the fact that multilingual BERT was trained on numerous languages it would take a lot of time to gather a good corpus for each language. This gives an opportunity for researchers from different countries to impove BERT as it's easier for them to gather corpus specifc to their region."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voL_xrhnhCD9",
        "colab_type": "text"
      },
      "source": [
        "On the contrary, trained BERT gives promising results. In the same sentences:\n",
        "\n",
        "'[CLS] Раз прийшов лис до [MASK] в гості , та й тхір його гарно погостив . [SEP]' and '[CLS] Той чоловік пригонить бички додому та й [MASK] : [SEP]' trained model outputs:\n",
        "\n",
        "'Раз прийшов лис до нього в гості, та й тхір його гарно погостив.' and  'Той чоловік пригонить бички додому та й каже:'\n",
        "\n",
        "These are pretty good results considering the fact that the original word was masked with only one MASK token. **As we can see, the model not only identifes the correct part of speech, but also outputs pronoun in the right gender.** This is due to the fact that BERT usses attention mechanis to infer context from surrounding words and thus, as a result, understands the context better and can make pretty good predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dbO8Vkhidtc",
        "colab_type": "text"
      },
      "source": [
        "However, even trained BERT sometimes fails to understand the context. It might be noticed that the model might place punctuation marks like \",\", or \".\" and \"-\" in place of the actual word. This result is not surprising since the corpus that the model was trained on consisted of many dialogues with those particular punctuation marks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5TfXxoCjZ7S",
        "colab_type": "text"
      },
      "source": [
        "<u> As a result, we can see that even when the model is trained on a small corpus the model outputs much better reults comparing to that of the original model. </u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvh4fcoqv_3B",
        "colab_type": "code",
        "outputId": "a89edf24-8f03-4892-cf74-15488efad6cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "text_sentences[0]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] Раз прийшов лис до [MASK] в гості , та й тхір його гарно погостив . [SEP]',\n",
              " '[CLS] Але приходить [MASK] тхір до лиса , а лис каже : [SEP]',\n",
              " '[CLS] Не маю я , куме [MASK] чим вас погостити , хіба би пішли зі мною тут до одного ґазди , у нього є цілий курник . [SEP]',\n",
              " '[CLS] [MASK] вони обоє і пішли шукати курей . [SEP]',\n",
              " '[CLS] Прийшли вони до того [MASK] . [SEP]',\n",
              " '[CLS] Каже [MASK] до тхора : [SEP]',\n",
              " '[CLS] Тут [MASK] курник . [SEP]',\n",
              " '[CLS] Ану , лізьте цією діркою досередини , а я буду на варті ; лиш собі , - каже , - не жалійте [MASK] бо господаря не шкода , у нього є досталь ! [SEP]',\n",
              " '[CLS] Тхір поліз досередини , лис заклав [MASK] дошкою , а сам підбіг під вікно та крикнув : [SEP]',\n",
              " '[CLS] [MASK] ґаздо ! [SEP]',\n",
              " '[CLS] [MASK] ! [SEP]',\n",
              " '[CLS] А ідіть - но до [MASK] , маєте зло\\xadдія . [SEP]',\n",
              " '[CLS] Та й [MASK] . [SEP]',\n",
              " '[CLS] Ґазда [MASK] , збудив синів та й повибігали надвір . [SEP]',\n",
              " '[CLS] [MASK] крик ! [SEP]',\n",
              " \"[CLS] А той тхір уже собі гарно під'їв та й [MASK] учув , що біда , а він далі . [SEP]\",\n",
              " '[CLS] Штовхнув дошку та й береться ліз\\xadти [MASK] а то не получається , бо став дуже товстий . [SEP]',\n",
              " '[CLS] А ті як взяли бука , як стануть бити , то тхір ледве [MASK] через дірку й утік ледве живий . [SEP]',\n",
              " '[CLS] Від того часу тхір з лисом [MASK] великій ненависті . [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPvDyMGpwEzG",
        "colab_type": "code",
        "outputId": "434e596a-b1ac-4b74-f778-a631365cd695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "cleaned_trained[0]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Раз прийшов лис до нього в гості, та й тхір його гарно погостив.',\n",
              " 'Але приходить до тхір до лиса, а лис каже: ',\n",
              " 'Не маю я, куме, чим вас погостити, хіба би пішли зі мною тут до одного ґазди, у нього є цілий курник.',\n",
              " 'Всі вони обоє і пішли шукати курей. ',\n",
              " 'Прийшли вони до того міста.',\n",
              " 'Каже - до тхора:',\n",
              " 'Тут - курник. ',\n",
              " 'Ану, лізьте цією діркою досередини, а я буду на варті ; лиш собі, - каже, - не жалійте, бо господаря не шкода, у нього є досталь! ',\n",
              " 'Тхір поліз досередини, лис заклав під дошкою, а сам підбіг під вікно та крикнув:',\n",
              " '- ґаздо!',\n",
              " '-! ',\n",
              " 'А ідіть - но до того, маєте злодія.',\n",
              " 'Та й.. ',\n",
              " 'Ґаздачі, збудив синів та й повибігали надвір. ',\n",
              " '- крик! ',\n",
              " \"А той тхір уже собі гарно під ' їв та й уже учув, що біда, а він далі.\",\n",
              " 'Штовхнув дошку та й береться лізти, а то не получається, бо став дуже товстий.',\n",
              " 'А ті як взяли бука, як стануть бити, то тхір ледве, через дірку й утік ледве живий. ',\n",
              " 'Від того часу тхір з лисом у великій ненависті.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJcbUvdMwI6r",
        "colab_type": "code",
        "outputId": "5baab901-c30f-4c3c-bc4e-f99dcec42060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "cleaned_basic[0]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Раз прийшов лис доu в гості, та й тхір його гарно погостив.',\n",
              " 'Але приходить до тхір до лиса, а лис каже: ',\n",
              " 'Не маю я, кумев чим вас погостити, хіба би пішли зі мною тут до одного ґазди, у нього є цілий курник.',\n",
              " '- вони обоє і пішли шукати курей. ',\n",
              " 'Прийшли вони до того..',\n",
              " 'Каже - до тхора:',\n",
              " 'Тут \" курник. ',\n",
              " 'Ану, лізьте цією діркою досередини, а я буду на варті ; лиш собі, - каже, - не жалійте, бо господаря не шкода, у нього є досталь! ',\n",
              " 'Тхір поліз досередини, лис заклав під дошкою, а сам підбіг під вікно та крикнув:',\n",
              " 'on ґаздо!',\n",
              " 'on! ',\n",
              " 'А ідіть - но до., маєте злодія.',\n",
              " 'Та йself. ',\n",
              " 'Ґазда., збудив синів та й повибігали надвір. ',\n",
              " 'on крик! ',\n",
              " \"А той тхір уже собі гарно під ' їв та й само учув, що біда, а він далі.\",\n",
              " 'Штовхнув дошку та й береться лізти, а то не получається, бо став дуже товстий.',\n",
              " 'А ті як взяли бука, як стануть бити, то тхір ледве, через дірку й утік ледве живий. ',\n",
              " 'Від того часу тхір з лисом. великій ненависті.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d5PXT7DwJL1",
        "colab_type": "code",
        "outputId": "2e01c715-9a85-4980-947b-2b377e7467ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "text_sentences[1]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] Був один чоловік бідний і любив [MASK] колись трош\\xadки напиватися . [SEP]',\n",
              " '[CLS] А до того бив дуже свою [MASK] . [SEP]',\n",
              " '[CLS] Але вона йому [MASK] не казала , лиш каже : [SEP]',\n",
              " '[CLS] Хай тобі пан Бог заплатить за мене , - а [MASK] нічого . [SEP]',\n",
              " '[CLS] Аж [MASK] разу каже той чоловік : [SEP]',\n",
              " '[CLS] Ану , я [MASK] до Бога за зарплатою . [SEP]',\n",
              " '[CLS] І [MASK] у ліс . [SEP]',\n",
              " '[CLS] Іде лісом , іде , дивиться : ангел летить з неба та [MASK] каже : [SEP]',\n",
              " '[CLS] А куди ти , чоловіче [MASK] ідеш ? [SEP]',\n",
              " '[CLS] [MASK] до Бога за зарплатою . [SEP]',\n",
              " '[CLS] [MASK] якою зарплатою ? питається ангел . [SEP]',\n",
              " '[CLS] А що ж , - каже той [MASK] , - я бив свою жінку , а вона все казала : \" Хай тобі пан Бог заплатить ! \" А я тепер іду до Бога , аби мені заплатив . [SEP]',\n",
              " '[CLS] Ангел [MASK] : [SEP]',\n",
              " '[CLS] Ходи сюди , [MASK] . [SEP]',\n",
              " '[CLS] І повів його за корч , а там пара бичків пасеться [MASK] і каже ангел : [SEP]',\n",
              " '[CLS] [MASK] тобі цю пару бичків та й іди додому . [SEP]',\n",
              " '[CLS] [MASK] сам пішов до неба . [SEP]',\n",
              " '[CLS] Той чоловік пригонить бички додому та й [MASK] : [SEP]',\n",
              " '[CLS] Дав мені [MASK] Бог пару бичків . [SEP]',\n",
              " '[CLS] А в того чоловіка [MASK] брат такий сильний багач , що пари йому не було на ціле село . [SEP]',\n",
              " '[CLS] Той багач був дуже лукавий і не любив свого брата , та й пішов до [MASK] . [SEP]',\n",
              " '[CLS] А тоді так було , що пани самі [MASK] робили . [SEP]',\n",
              " '[CLS] [MASK] каже панові так : [SEP]',\n",
              " '[CLS] Мій брат казав , щоби панський лан тими бичка\\xadми , що він має , то [MASK] за одну ніч виорав , засіяв і заволочив . [SEP]',\n",
              " '[CLS] А то був такий лан великий , що двадцять плугів треба було [MASK] аби за десять днів виорати . [SEP]',\n",
              " '[CLS] Пан зараз [MASK] його закликати і каже : [SEP]',\n",
              " '[CLS] Іди [MASK] , аби мені за цілу ніч виорав , бо як не виореш , то скажу тебе нагайками бити . [SEP]',\n",
              " '[CLS] І пішов бідний [MASK] . [SEP]',\n",
              " '[CLS] [MASK] додому та й плаче . [SEP]',\n",
              " '[CLS] Входить він до стайні , дивиться [MASK] бички та й каже : [SEP]',\n",
              " '[CLS] [MASK] тут є орати ? [SEP]',\n",
              " '[CLS] А бички [MASK] : [SEP]',\n",
              " '[CLS] Цить [MASK] не плач , не журися , бери та лагодь плуг , та й підемо орати . [SEP]',\n",
              " '[CLS] Дав він бичкам [MASK] , а сам лагодить плуг . [SEP]',\n",
              " '[CLS] Як бички вже попоїли , він [MASK] та й їде орати . [SEP]',\n",
              " '[CLS] Господи , він лиш одну [MASK] загнав орати , а то десять зараз іде . [SEP]',\n",
              " '[CLS] І за пару годин стала рілля на [MASK] лану . [SEP]',\n",
              " '[CLS] За годину заволочив і [MASK] . [SEP]',\n",
              " '[CLS] Так йому Бог допоміг , [MASK] він одної ночі все зробив . [SEP]',\n",
              " '[CLS] Рано пан виходить , дивиться , що вже зроблено , [MASK] каже : [SEP]',\n",
              " '[CLS] Це [MASK] мудрий . [SEP]',\n",
              " '[CLS] А брат його знов приходить до пана [MASK] й каже : [SEP]',\n",
              " '[CLS] Пане мій , брат казав , [MASK] тими бичками пана , паню і всю панську родину де яка є , і мене , і мою жінку , і всю мою родину завіз би до пекла . [SEP]',\n",
              " '[CLS] [MASK] зараз пан наказав робити такий великий віз , аби вся родина помістилася . [SEP]',\n",
              " '[CLS] [MASK] зараз зробили . [SEP]',\n",
              " '[CLS] Та й пан кличе того [MASK] Івана та й каже : [SEP]',\n",
              " '[CLS] Ану , Іване , запрягай свої [MASK] та й будеш нас везти до пекла . [SEP]',\n",
              " '[CLS] [MASK] бідний Іван бере та й запрягає свої бички , та й везе всю родину панську і родину свого брата . [SEP]',\n",
              " '[CLS] Їде він , [MASK] попід гору таку високу , що страх . [SEP]',\n",
              " '[CLS] А ті бички йому шепчуть і кажуть : \" їдь право на [MASK] \" . [SEP]',\n",
              " '[CLS] А та гора над [MASK] водою . [SEP]',\n",
              " '[CLS] Ті бички [MASK] тягнуть вгору , аж стогнуть . [SEP]',\n",
              " '[CLS] Та й знов йому шепчуть і кажуть : \" візьми та [MASK] витягни притику \" . [SEP]',\n",
              " '[CLS] Він як витягнув притику , а віз стрімголов полетів у долину , і порозбивалася вся роди\\xadна , [MASK] й бички полетіли до неба . [SEP]',\n",
              " '[CLS] А бідний Іван засів собі в панськім дворі свого брата [MASK] жив собі по - панськи аж до смерті . [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A-zadHswJTo",
        "colab_type": "code",
        "outputId": "00936254-d28e-4d48-a7c5-bd3f00211066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "cleaned_trained[1]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Був один чоловік бідний і любивший колись трошки напиватися. ',\n",
              " 'А до того бив дуже свою..',\n",
              " 'Але вона йому, не казала, лиш каже: ',\n",
              " 'Хай тобі пан Бог заплатить за мене, - а. нічого.',\n",
              " 'Аж, разу каже той чоловік: ',\n",
              " 'Ану, я, до Бога за зарплатою. ',\n",
              " 'І - у ліс. ',\n",
              " 'Іде лісом, іде, дивиться: ангел летить з неба та й каже:',\n",
              " 'А куди ти, чоловіче, ідеш? ',\n",
              " '- до Бога за зарплатою. ',\n",
              " '- якою зарплатою? питається ангел. ',\n",
              " 'А що ж, - каже той чоловік, - я бив свою жінку, а вона все казала: \" Хай тобі пан Бог заплатить! \" А я тепер іду до Бога, аби мені заплатив.',\n",
              " 'Ангел -: ',\n",
              " 'Ходи сюди, а. ',\n",
              " 'І повів його за корч, а там пара бичків пасеться, і каже ангел:',\n",
              " '- тобі цю пару бичків та й іди додому. ',\n",
              " '- сам пішов до неба. ',\n",
              " 'Той чоловік пригонить бички додому та й каже:',\n",
              " 'Дав мені - Бог пару бичків. ',\n",
              " 'А в того чоловіка мав брат такий сильний багач, що пари йому не було на ціле село. ',\n",
              " 'Той багач був дуже лукавий і не любив свого брата, та й пішов до міста.',\n",
              " 'А тоді так було, що пани самі не робили.',\n",
              " '- каже панові так:',\n",
              " 'Мій брат казав, щоби панський лан тими бичками, що він має, то він за одну ніч виорав, засіяв і заволочив.',\n",
              " 'А то був такий лан великий, що двадцять плугів треба було, аби за десять днів виорати. ',\n",
              " 'Пан заразi його закликати і каже:',\n",
              " 'Іди так, аби мені за цілу ніч виорав, бо як не виореш, то скажу тебе нагайками бити.',\n",
              " 'І пішов бідний батько.',\n",
              " '- додому та й плаче.',\n",
              " 'Входить він до стайні, дивиться до бички та й каже:',\n",
              " '- тут є орати? ',\n",
              " 'А бички -:',\n",
              " 'Цить, не плач, не журися, бери та лагодь плуг, та й підемо орати.',\n",
              " 'Дав він бичкам -, а сам лагодить плуг.',\n",
              " 'Як бички вже попоїли, він, та й їде орати.',\n",
              " 'Господи, він лиш одну зараз загнав орати, а то десять зараз іде.',\n",
              " 'І за пару годин стала рілля на місці лану.',\n",
              " 'За годину заволочив і..',\n",
              " 'Так йому Бог допоміг, що він одної ночі все зробив. ',\n",
              " 'Рано пан виходить, дивиться, що вже зроблено, і каже:',\n",
              " 'Це - мудрий. ',\n",
              " 'А брат його знов приходить до пана та й каже: ',\n",
              " 'Пане мій, брат казав, що тими бичками пана, паню і всю панську родину де яка є, і мене, і мою жінку, і всю мою родину завіз би до пекла.',\n",
              " '. зараз пан наказав робити такий великий віз, аби вся родина помістилася.',\n",
              " '- зараз зробили. ',\n",
              " 'Та й пан кличе того й Івана та й каже: ',\n",
              " 'Ану, Іване, запрягай свої, та й будеш нас везти до пекла. ',\n",
              " '- бідний Іван бере та й запрягає свої бички, та й везе всю родину панську і родину свого брата.',\n",
              " 'Їде він, як попід гору таку високу, що страх.',\n",
              " 'А ті бички йому шепчуть і кажуть: \" їдь право на честь \". ',\n",
              " 'А та гора надi водою. ',\n",
              " 'Ті бички були тягнуть вгору, аж стогнуть.',\n",
              " 'Та й знов йому шепчуть і кажуть: \" візьми та й витягни притику \".',\n",
              " 'Він як витягнув притику, а віз стрімголов полетів у долину, і порозбивалася вся родина, та й бички полетіли до неба.',\n",
              " 'А бідний Іван засів собі в панськім дворі свого брата, жив собі по - панськи аж до смерті. ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwgrz0HjwJaA",
        "colab_type": "code",
        "outputId": "785df4ed-19ac-4c96-fc4f-55d4cda711dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "cleaned_basic[1]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Був один чоловік бідний і любивy колись трошки напиватися. ',\n",
              " 'А до того бив дуже свою..',\n",
              " 'Але вона йому mai не казала, лиш каже: ',\n",
              " 'Хай тобі пан Бог заплатить за мене, - а не нічого.',\n",
              " 'Аж tale разу каже той чоловік: ',\n",
              " 'Ану, я, до Бога за зарплатою. ',\n",
              " 'І. у ліс. ',\n",
              " 'Іде лісом, іде, дивиться: ангел летить з неба та, каже:',\n",
              " 'А куди ти, чоловіче - ідеш? ',\n",
              " '\" до Бога за зарплатою. ',\n",
              " ', якою зарплатою? питається ангел. ',\n",
              " 'А що ж, - каже той же, - я бив свою жінку, а вона все казала: \" Хай тобі пан Бог заплатить! \" А я тепер іду до Бога, аби мені заплатив.',\n",
              " 'Ангел \": ',\n",
              " 'Ходи сюди, m. ',\n",
              " 'І повів його за корч, а там пара бичків пасеться, і каже ангел:',\n",
              " ', тобі цю пару бичків та й іди додому. ',\n",
              " 'a сам пішов до неба. ',\n",
              " 'Той чоловік пригонить бички додому та йо:',\n",
              " 'Дав мені \" Бог пару бичків. ',\n",
              " 'А в того чоловіка був брат такий сильний багач, що пари йому не було на ціле село. ',\n",
              " 'Той багач був дуже лукавий і не любив свого брата, та й пішов до..',\n",
              " 'А тоді так було, що пани самі себе робили.',\n",
              " 'ei каже панові так:',\n",
              " 'Мій брат казав, щоби панський лан тими бичками, що він має, то, за одну ніч виорав, засіяв і заволочив.',\n",
              " 'А то був такий лан великий, що двадцять плугів треба було, аби за десять днів виорати. ',\n",
              " 'Пан заразe його закликати і каже:',\n",
              " 'Ідиш, аби мені за цілу ніч виорав, бо як не виореш, то скажу тебе нагайками бити.',\n",
              " 'І пішов бідний..',\n",
              " '\" додому та й плаче.',\n",
              " 'Входить він до стайні, дивиться до бички та й каже:',\n",
              " 'din тут є орати? ',\n",
              " 'А бичкиn:',\n",
              " 'Цить, не плач, не журися, бери та лагодь плуг, та й підемо орати.',\n",
              " 'Дав він бичкам., а сам лагодить плуг.',\n",
              " 'Як бички вже попоїли, він - та й їде орати.',\n",
              " 'Господи, він лиш одну, загнав орати, а то десять зараз іде.',\n",
              " 'І за пару годин стала рілля на 1 лану.',\n",
              " 'За годину заволочив і..',\n",
              " 'Так йому Бог допоміг, and він одної ночі все зробив. ',\n",
              " 'Рано пан виходить, дивиться, що вже зроблено,, каже:',\n",
              " 'Це \" мудрий. ',\n",
              " 'А брат його знов приходить до пана, й каже: ',\n",
              " 'Пане мій, брат казав, що тими бичками пана, паню і всю панську родину де яка є, і мене, і мою жінку, і всю мою родину завіз би до пекла.',\n",
              " '. зараз пан наказав робити такий великий віз, аби вся родина помістилася.',\n",
              " '\" зараз зробили. ',\n",
              " 'Та й пан кличе того, Івана та й каже: ',\n",
              " 'Ану, Іване, запрягай свої, та й будеш нас везти до пекла. ',\n",
              " '... бідний Іван бере та й запрягає свої бички, та й везе всю родину панську і родину свого брата.',\n",
              " 'Їде він, a попід гору таку високу, що страх.',\n",
              " 'А ті бички йому шепчуть і кажуть: \" їдь право на право \". ',\n",
              " 'А та гора над \" водою. ',\n",
              " 'Ті бички, тягнуть вгору, аж стогнуть.',\n",
              " 'Та й знов йому шепчуть і кажуть: \" візьми та та витягни притику \".',\n",
              " 'Він як витягнув притику, а віз стрімголов полетів у долину, і порозбивалася вся родина, а й бички полетіли до неба.',\n",
              " 'А бідний Іван засів собі в панськім дворі свого брата і жив собі по - панськи аж до смерті. ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCZC4y5XwJlh",
        "colab_type": "code",
        "outputId": "3ae25a3d-e9eb-4065-d83d-02f065f3b494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "text_sentences[2]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] Жив [MASK] у лісі маленький їжачок . [SEP]',\n",
              " '[CLS] Була у нього своя [MASK] . [SEP]',\n",
              " '[CLS] Збирав [MASK] яблучка , грибочки , ягоди . [SEP]',\n",
              " '[CLS] Тільки було йому дуже [MASK] . [SEP]',\n",
              " '[CLS] От одного разу вирішив він знайти собі [MASK] . [SEP]',\n",
              " '[CLS] Вийшов він із хатинки [MASK] й пішов до лісу . [SEP]',\n",
              " '[CLS] Блукав їжачок дуже довго , але тільки ніхто не захотів [MASK] ним товаришувати . [SEP]',\n",
              " '[CLS] Лисичка сказала , що він [MASK] колючий . [SEP]',\n",
              " '[CLS] Зайчик сказав [MASK] що їжачок швидко не бігає і з ним йому буде не цікаво , а ведмідь зовсім не захотів з ним розмовляти . [SEP]',\n",
              " '[CLS] Розплакався [MASK] та й пішов собі стежкою . [SEP]',\n",
              " '[CLS] Ішов [MASK] , ішов та й дійшов до людського саду . [SEP]',\n",
              " '[CLS] Бачить , [MASK] там яблучок на землю багато - багато нападало . [SEP]',\n",
              " '[CLS] От їжачок і подумав : \" Назбираю я [MASK] собі додому \" . [SEP]',\n",
              " '[CLS] Тільки почав [MASK] , аж раптом почув тріск сухої гілки за спиною . [SEP]',\n",
              " '[CLS] Коли їжачок обернувся , побачив маленьку дівчинку і дуже [MASK] . [SEP]',\n",
              " '[CLS] Але дівчинка й не збиралася [MASK] його . [SEP]',\n",
              " '[CLS] Вона [MASK] назбирати яблучок їжачкові на голочки . [SEP]',\n",
              " '[CLS] Він дуже [MASK] і запропонував дівчинці погратися . [SEP]',\n",
              " '[CLS] Довго вони гралися , доки дівчинку не [MASK] мати . [SEP]',\n",
              " '[CLS] Але перед тим , як піти , дівчинка запропонувала їжачкові свою дружбу , [MASK] запросила завтра до саду погратися . [SEP]',\n",
              " '[CLS] Їжачок дуже зрадів [MASK] погодився . [SEP]',\n",
              " '[CLS] Коли він прийшов додому , ще довго не міг [MASK] . [SEP]',\n",
              " '[CLS] Все [MASK] і мріяв , як завтра вони будуть бавитись . [SEP]',\n",
              " '[CLS] І відтоді вони зустрічалися кожного дня і [MASK] в садку . [SEP]',\n",
              " '[CLS] Ось так наша маленька тваринка [MASK] собі яблучок на зиму , а найголовніше - знайшла собі справжнього друга . [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXXtWQW2wJsY",
        "colab_type": "code",
        "outputId": "6513dfb9-52cf-48eb-880c-3b60c0271ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "cleaned_trained[2]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Жив, у лісі маленький їжачок.',\n",
              " 'Була у нього своя..',\n",
              " 'Збирав, яблучка, грибочки, ягоди. ',\n",
              " 'Тільки було йому дуже.. ',\n",
              " 'От одного разу вирішив він знайти собі.. ',\n",
              " 'Вийшов він із хатинки та й пішов до лісу.',\n",
              " 'Блукав їжачок дуже довго, але тільки ніхто не захотів з ним товаришувати.',\n",
              " 'Лисичка сказала, що він був колючий.',\n",
              " 'Зайчик сказав, що їжачок швидко не бігає і з ним йому буде не цікаво, а ведмідь зовсім не захотів з ним розмовляти.',\n",
              " 'Розплакався, та й пішов собі стежкою. ',\n",
              " 'Ішов та, ішов та й дійшов до людського саду. ',\n",
              " 'Бачить, що там яблучок на землю багато - багато нападало.',\n",
              " 'От їжачок і подумав: \" Назбираю я - собі додому \".',\n",
              " 'Тільки почав час, аж раптом почув тріск сухої гілки за спиною.',\n",
              " 'Коли їжачок обернувся, побачив маленьку дівчинку і дуже великі.',\n",
              " 'Але дівчинка й не збиралася на його.',\n",
              " 'Вона могла назбирати яблучок їжачкові на голочки. ',\n",
              " 'Він дуже було і запропонував дівчинці погратися. ',\n",
              " 'Довго вони гралися, доки дівчинку не було мати.',\n",
              " 'Але перед тим, як піти, дівчинка запропонувала їжачкові свою дружбу, і запросила завтра до саду погратися. ',\n",
              " 'Їжачок дуже зрадів і погодився. ',\n",
              " 'Коли він прийшов додому, ще довго не міг..',\n",
              " 'Все - і мріяв, як завтра вони будуть бавитись.',\n",
              " 'І відтоді вони зустрічалися кожного дня і навіть в садку. ',\n",
              " 'Ось так наша маленька тваринка дала собі яблучок на зиму, а найголовніше - знайшла собі справжнього друга.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akLpz8uvwJyE",
        "colab_type": "code",
        "outputId": "9ca87087-7fad-441e-dbc0-c60a12a19397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "cleaned_basic[2]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Жив. у лісі маленький їжачок.',\n",
              " 'Була у нього своя \".',\n",
              " 'Збирав. яблучка, грибочки, ягоди. ',\n",
              " 'Тільки було йому дуже \". ',\n",
              " 'От одного разу вирішив він знайти собі.. ',\n",
              " 'Вийшов він із хатинки. й пішов до лісу.',\n",
              " 'Блукав їжачок дуже довго, але тільки ніхто не захотів с ним товаришувати.',\n",
              " 'Лисичка сказала, що він був колючий.',\n",
              " 'Зайчик сказав, що їжачок швидко не бігає і з ним йому буде не цікаво, а ведмідь зовсім не захотів з ним розмовляти.',\n",
              " 'Розплакався. та й пішов собі стежкою. ',\n",
              " 'Ішов та, ішов та й дійшов до людського саду. ',\n",
              " 'Бачить, \" там яблучок на землю багато - багато нападало.',\n",
              " 'От їжачок і подумав: \" Назбираю я, собі додому \".',\n",
              " 'Тільки почавt, аж раптом почув тріск сухої гілки за спиною.',\n",
              " 'Коли їжачок обернувся, побачив маленьку дівчинку і дуже..',\n",
              " 'Але дівчинка й не збиралася o його.',\n",
              " 'Вона може назбирати яблучок їжачкові на голочки. ',\n",
              " 'Він дуже lent і запропонував дівчинці погратися. ',\n",
              " 'Довго вони гралися, доки дівчинку не - мати.',\n",
              " 'Але перед тим, як піти, дівчинка запропонувала їжачкові свою дружбу, і запросила завтра до саду погратися. ',\n",
              " 'Їжачок дуже зрадів, погодився. ',\n",
              " 'Коли він прийшов додому, ще довго не міг..',\n",
              " 'Все, і мріяв, як завтра вони будуть бавитись.',\n",
              " 'І відтоді вони зустрічалися кожного дня і навіть в садку. ',\n",
              " 'Ось так наша маленька тваринка дала собі яблучок на зиму, а найголовніше - знайшла собі справжнього друга.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqIaRlrwjo-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicted_texts = []\n",
        "\n",
        "# for i in range(len(text_sentences)):\n",
        "#   print(\"Processing text: \", i)\n",
        "#   text = text_sentences[i]\n",
        "#   segment_ids, indexed_tokens = get_segments_and_tokens(text)\n",
        "#   print(\"Recieved segments and indexed_tokens\")\n",
        "\n",
        "#   segments_tensor = (torch.tensor(segment_ids))\n",
        "#   tokens_tensor = torch.tensor(indexed_tokens)\n",
        "\n",
        "#   print(\"Model started\")\n",
        "\n",
        "#   input_length = len(segments_tensor)\n",
        "#   print(input_length)\n",
        "\n",
        "#   logits = []\n",
        "#   text = []\n",
        "#   for j in range(0, input_length, 10):\n",
        "\n",
        "#     left = j\n",
        "#     right = j+10\n",
        "#     if (right > input_length):\n",
        "#       right = input_length\n",
        "\n",
        "#     if (left == input_length - 1):\n",
        "#       right = input_length\n",
        "\n",
        "#     segments_portion = segment_ids[left:right]\n",
        "#     tokens_portion = indexed_tokens[left:right]\n",
        "#     segments_tensor = (torch.tensor(segments_portion))\n",
        "#     tokens_tensor = torch.tensor(tokens_portion)\n",
        "\n",
        "#     tokens_tensor = tokens_tensor.to(device)\n",
        "#     segments_tensor = segments_tensor.to(device)\n",
        "\n",
        "#     print(\"Processing from {} to {}\".format(left, right))\n",
        "#     with torch.no_grad():\n",
        "#         masked_lm_logits_scores, _ = maskedLM_model(tokens_tensor, segments_tensor)\n",
        "#         # logits.append(masked_lm_logits_scores)\n",
        "    \n",
        "#   # logits = np.stack(logits, axis = 0)\n",
        "#     print(\"Model finished\")\n",
        "\n",
        "#     predictions = masked_lm_logits_scores\n",
        "#     inference = indexed_tokens[left:right]\n",
        "\n",
        "#     new_inference = make_inference(predictions, inference)\n",
        "#     print(\"Recieved new inference\")\n",
        "\n",
        "#     predicted_sentences = convert_inference_to_sentences(new_inference)\n",
        "#     print(\"Recieved predicted_sentences\")\n",
        "\n",
        "#     text += predicted_sentences\n",
        "\n",
        "\n",
        "#   predicted_texts.append(text)\n",
        "#   print(\"Appended new sentences for: \", i)\n",
        "#   print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGcD5P42s7lW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saved_predicted = predicted_texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRXMQrtvsgIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned = []\n",
        "# for j in range(len(saved_predicted)):\n",
        "#     text_to_work = saved_predicted[j]\n",
        "#     for i in range(len(text_to_work)):\n",
        "#       sentence = text_to_work[i]\n",
        "#       sentence = sentence.replace(\" ?\", \"?\")\n",
        "#       sentence = sentence.replace(\" !\", \"!\")\n",
        "#       sentence = sentence.replace(\" :\", \":\")\n",
        "#       sentence = sentence.replace(\"  \", \"\")\n",
        "#       sentence = sentence.replace(\" ,\", \",\")\n",
        "#       sentence = sentence.replace(\" .\", \".\")\n",
        "#       text_to_work[i] = sentence\n",
        "#     cleaned.append(text_to_work)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0vGTFWhtuBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned_texts = []\n",
        "# for clean in cleaned:\n",
        "#   cleaned_texts.append(\" \".join(clean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS_em5Rvupy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned_texts[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnn2OTqBq95O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_sentences[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERi4Xu1iqojs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned_texts[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkGMwLlSq-k4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_sentences[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuS0WYx7qrJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned_texts[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss-b6iGCq_21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_sentences[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdzkKWMURSGc",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Story Generation Based On Next Sentence Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mm-433BkLFd",
        "colab_type": "text"
      },
      "source": [
        "The second part of the project was primiraly focused on how good of a story can BERT generated given outputs from the previous part. This is definitely a hard task for the model since its inputs are outputs of the trained model from the previous step. This means that the context of the stories from the previous step might be somewhat obscured and thus lead to not so surprising results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XniclFLqk1v3",
        "colab_type": "text"
      },
      "source": [
        "As mentioned, the inputs for this part are the outputs of the trained model from the previous step. Here we again break each story into sentences and choose first sentence of the first story as our foundation. After that we cansider each pair of sentences consisting of our first sentene and all other sentences. After this we choose a pair with the highest probability of the second sentence being a continuation of the first one which gives us a second sentence. The process now repeats with the second sentence to generate the third. The length of the story is: MIN(length of the 1st story, length of the 2nd + 3rd stories). This is a computation heavy task, so GPU might be a good choice for this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-VsPsSbRl2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM, BertForPreTraining\n",
        "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
        "from random import random, randrange, randint, shuffle, choice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCjsrAj7m4O0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepared_second = []\n",
        "for clean in cleaned_trained:\n",
        "  prepared_second.append(\" \".join(clean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOziBbYBfT3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_text = prepared_second[0]\n",
        "second_text = prepared_second[1]\n",
        "third_text = prepared_second[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybOCgiWbR10I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = [first_text, second_text, third_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_qPtRae-C-jJ",
        "colab": {}
      },
      "source": [
        "# Cleans texts by substituting similar punctuation marks with a single standard\n",
        "def clean_texts(texts):\n",
        "\n",
        "  dashes = {'–', '—', '―', '~'} # replace with -\n",
        "  special_symbols = {'№', '_', '<', '>', '|', ']', '*', '[', '^', '&'} # replace with \"\"\n",
        "  apostrophes = {'’', '‘'} # replace with '\n",
        "  direct_speech = {'“', '»', '«'} # replace with '\"'\n",
        "  three_dots = {'…'} # replace with '.\n",
        "\n",
        "  counter = 0\n",
        "  for i in range(len(texts)):\n",
        "    print(\"Processing text: \", i)\n",
        "    text = texts[i]\n",
        "    words = []\n",
        "    tokenized_words = tokenize_uk.tokenize_words(text)\n",
        "    for word in tokenized_words:\n",
        "      added = False\n",
        "\n",
        "      for dash in dashes:\n",
        "        if (dash in word):\n",
        "          new_word = word.replace(dash, \"-\")\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "\n",
        "      for special_symbol in special_symbols:\n",
        "        if(special_symbol in word):\n",
        "          new_word = word.replace(special_symbol, \"\")\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "      \n",
        "      for apostrophe in apostrophes:\n",
        "        if (apostrophe in word):\n",
        "          new_word = word.replace(apostrophe, \"'\")\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "\n",
        "      for direct in direct_speech:\n",
        "        if (direct in word):\n",
        "          new_word = word.replace(direct, '\"')\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "      \n",
        "      for dots in three_dots:\n",
        "        if (dots in word):\n",
        "          counter += 1\n",
        "          new_word = word.replace(dots, '.')\n",
        "          words.append(new_word)\n",
        "          added = True\n",
        "          continue\n",
        "      if (not added):\n",
        "        words.append(word)\n",
        "    reconstructed_text = \" \".join(words)\n",
        "    texts[i] = reconstructed_text \n",
        "  return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "05620018-478f-43a6-864a-cb7933b7cee7",
        "id": "65_90j7qC-jL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "texts = clean_texts(texts)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing text:  0\n",
            "Processing text:  1\n",
            "Processing text:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8lNqlZclC-jN",
        "colab": {}
      },
      "source": [
        "# Prepares sentences for tokenization\n",
        "def prepare_for_tokenization(texts):\n",
        "  new_texts = []\n",
        "  for i in range(len(texts)):\n",
        "    text = texts[i]\n",
        "    text = text.replace(\"?\", \"?.\")\n",
        "    text = text.replace(\"!\", \"!.\")\n",
        "    text = text.replace(\":\", \":.\")\n",
        "    text = text.replace(\". -\", \". \")\n",
        "    new_texts.append(text)\n",
        "  return new_texts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f0f155e3-0550-46af-deec-4076fb42829d",
        "id": "mDi6jkLsC-jP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_texts = prepare_for_tokenization(texts)\n",
        "print(\"Number of texts: \", len(new_texts))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of texts:  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O2lbFBvLC-jQ",
        "colab": {}
      },
      "source": [
        "# Tokenizes texts into sentences \n",
        "def tokenize_texts_to_sentences(texts):\n",
        "  text_sentences = []\n",
        "  for text in texts:\n",
        "    text_to_add = []\n",
        "    text_to_add += tokenize_uk.tokenize_sents(text)\n",
        "    text_sentences.append(text_to_add)\n",
        "  return text_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fdfc7402-232c-4732-f560-9ad27d8adc03",
        "id": "hXnd7hZEC-jR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text_sentences = tokenize_texts_to_sentences(new_texts)\n",
        "print(\"Number of tokenized texts: \", len(text_sentences))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokenized texts:  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "aa4b71a2-a55f-4504-e09a-366dc88e8434",
        "id": "snGgj9R9C-jS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for i in range(len(text_sentences)):\n",
        "  print(i, \" contains \", len(text_sentences[i]), \" sentences\")"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  contains  16  sentences\n",
            "1  contains  45  sentences\n",
            "2  contains  25  sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9DshrkUTC-jU",
        "colab": {}
      },
      "source": [
        "# print(new_texts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RTDzG6N6C-jV",
        "colab": {}
      },
      "source": [
        "# Cleans texts after splitting them to sentences\n",
        "def clean_after_tokenization(text_sentences):\n",
        "  cleaned = []\n",
        "\n",
        "  for j in range(len(text_sentences)):\n",
        "    text_to_work = text_sentences[j]\n",
        "    for i in range(len(text_to_work)):\n",
        "      sentence = text_to_work[i]\n",
        "      sentence = sentence.replace(\"?.\", \"?\")\n",
        "      sentence = sentence.replace(\"!.\", \"!\")\n",
        "      sentence = sentence.replace(\":.\", \":\")\n",
        "      text_to_work[i] = sentence\n",
        "    cleaned.append(text_to_work)\n",
        "  return cleaned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WHaebxS6C-jW",
        "colab": {}
      },
      "source": [
        "text_sentences = clean_after_tokenization(text_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f9131e2e-429a-4de4-9248-c163ee3fcc62",
        "id": "D9TmCxC2C-jX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for i in range(len(text_sentences)):\n",
        "  print(i, \" contains \", len(text_sentences[i]), \" sentences\")"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  contains  16  sentences\n",
            "1  contains  45  sentences\n",
            "2  contains  25  sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X19injDfDZvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes generated story length\n",
        "def get_story_length(text_sentences):\n",
        "  story_length = len(text_sentences[0])\n",
        "  length_of_stories = len(text_sentences[1]) + len(text_sentences[2])\n",
        "  if (story_length > length_of_stories):\n",
        "    story_length = length_of_stories\n",
        "  return story_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsuEPFRADmHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creates list of sentences for consideration\n",
        "def get_sentences_for_consideration(text_sentences):\n",
        "  sentences_for_considerartion = text_sentences[0][1:]\n",
        "  sentences_for_considerartion += text_sentences[1]\n",
        "  sentences_for_considerartion += text_sentences[2]\n",
        "\n",
        "  return sentences_for_considerartion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NOVCVWeK-x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns segment ids and preprared sentence pairs\n",
        "def get_segment_ids_and_prepared_text(sentences_for_considerartion):\n",
        "\n",
        "  prepared_text = []\n",
        "  segment_ids = []\n",
        "  for j in range(len(sentences_for_considerartion)):\n",
        "    first_sentence = \"[CLS] \" + story[current_sentence_index] + \" [SEP] \" \n",
        "    second_sentence = sentences_for_considerartion[j] + \" [SEP] \" \n",
        "    segment_ids.append([0]* len(first_sentence) + [1]*len(second_sentence))\n",
        "    prepared_text.append(first_sentence + second_sentence)\n",
        "  \n",
        "  return segment_ids, prepared_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aVg57NULhZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns length of longest sentence pair\n",
        "def get_longest_sequence(prepared_text):\n",
        "  longest_sentence = 0\n",
        "  for sent in prepared_text:\n",
        "      if (len(sent) > longest_sentence):\n",
        "        longest_sentence = len(sent)\n",
        "  return longest_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI78ICVULxZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns indexed tokens and segment ids\n",
        "def get_indexed_tokens_and_segment_ids(prepared_text, segment_ids):\n",
        "  indexed_tokens = []\n",
        "  segment_ids = segment_ids\n",
        "\n",
        "  for j in range(len(prepared_text)):\n",
        "    tokenized_text.append(trained_tokenizer.tokenize(prepared_text[j]))\n",
        "    indexed_tokens.append(trained_tokenizer.convert_tokens_to_ids(tokenized_text[j]))\n",
        "  \n",
        "  indexed_tokens = pad_sequences(indexed_tokens, maxlen=longest_sentence, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  segment_ids = pad_sequences(segment_ids, maxlen=longest_sentence, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  return indexed_tokens, segment_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oncod1MVMi4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns index for sentence from sentences_for_consideraton to add to story\n",
        "def get_index_for_adding_sentence(logits):\n",
        "  real_logits = []\n",
        "  for logit in logits:\n",
        "    real_logits.append([logit[0], logit[1]])\n",
        "\n",
        "  real_logits = np.asarray(real_logits)\n",
        "\n",
        "  indices = []\n",
        "  logits_list = []\n",
        "  for i in range(real_logits.shape[0]):\n",
        "    if (np.argmax(real_logits[i]) == 0):\n",
        "      indices.append(i)\n",
        "      logits_list.append(real_logits[i][0]) # first trial showed that it's better of without abs function!\n",
        "  \n",
        "  if (len(logits_list) == 0):\n",
        "    index = np.argmax(logits, axis = 0)[0]\n",
        "  else:\n",
        "    index = indices[np.argmax(logits_list)]\n",
        "  return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOKp_yD5c_X-",
        "colab_type": "code",
        "outputId": "dd26eb2f-53fc-4862-e30e-200708ed9128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "story = [text_sentences[0][0]]\n",
        "current_sentence_index = 0\n",
        "\n",
        "story_length = get_story_length(text_sentences)\n",
        "print(\"Story length: \", story_length)\n",
        "sentences_for_considerartion = get_sentences_for_consideration(text_sentences)\n",
        "print(\"Number of sentences for consideration: \", len(sentences_for_considerartion))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Story length:  16\n",
            "Number of sentences for consideration:  85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VnEanURFIZK",
        "colab_type": "code",
        "outputId": "54888bc3-b78e-46d7-93ea-e8a02332e103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(current_sentence_index, story_length-1):\n",
        "  \n",
        "  tokenized_text = []\n",
        "  indexed_tokens = []\n",
        "  prepared_text = []\n",
        "  segment_ids = []\n",
        "\n",
        "  print(\"Current sentence index: \", current_sentence_index)\n",
        "  segment_ids, prepared_text = get_segment_ids_and_prepared_text(sentences_for_considerartion)\n",
        "\n",
        "  longest_sentence = get_longest_sequence(prepared_text)\n",
        "  print(\"Longest sentence is: \", longest_sentence)\n",
        "  \n",
        "  indexed_tokens, segment_ids = get_indexed_tokens_and_segment_ids(prepared_text, segment_ids)\n",
        "\n",
        "  input_length = len(segment_ids)\n",
        "  logits = []\n",
        "  step = 10\n",
        "\n",
        "  for j in range(0, input_length, step):\n",
        "\n",
        "    left = j\n",
        "    right = j+step\n",
        "\n",
        "    if (right > input_length):\n",
        "      right = input_length\n",
        "\n",
        "    if (left == input_length - 1):\n",
        "      right = input_length\n",
        "\n",
        "    segments_portion = segment_ids[left:right]\n",
        "    tokens_portion = indexed_tokens[left:right]\n",
        "    segments_tensor = (torch.tensor(segments_portion))\n",
        "    tokens_tensor = torch.tensor(tokens_portion)\n",
        "\n",
        "    segments_tensor = segments_tensor.to(device)\n",
        "    tokens_tensor = tokens_tensor.to(device)\n",
        "\n",
        "    print(\"Processing from {} to {}\".format(left, right))\n",
        "    with torch.no_grad():\n",
        "        _, seq_relationship_logits = trained_model(tokens_tensor, segments_tensor)\n",
        "        logits += list(seq_relationship_logits.cpu().numpy())\n",
        "  \n",
        "  index = get_index_for_adding_sentence(logits)\n",
        "  print(index)\n",
        "  print(\"Adding sentence: \", sentences_for_considerartion[index])\n",
        "  story.append(sentences_for_considerartion[index])\n",
        "\n",
        "  del sentences_for_considerartion[index]\n",
        "\n",
        "  print(\"Number of sentences for consideration: \", len(sentences_for_considerartion))\n",
        "\n",
        "  current_sentence_index += 1"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current sentence index:  0\n",
            "Longest sentence is:  322\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 80\n",
            "Processing from 80 to 85\n",
            "5\n",
            "Adding sentence:  Тут - курник .\n",
            "Number of sentences for consideration:  84\n",
            "Current sentence index:  1\n",
            "Longest sentence is:  270\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 80\n",
            "Processing from 80 to 84\n",
            "45\n",
            "Adding sentence:  Так йому Бог допоміг , що він одної ночі все зробив .\n",
            "Number of sentences for consideration:  83\n",
            "Current sentence index:  2\n",
            "Longest sentence is:  309\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 80\n",
            "Processing from 80 to 83\n",
            "8\n",
            "Adding sentence:  Та й ..\n",
            "Number of sentences for consideration:  82\n",
            "Current sentence index:  3\n",
            "Longest sentence is:  263\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 80\n",
            "Processing from 80 to 82\n",
            "23\n",
            "Adding sentence:  Ангел - :\n",
            "Number of sentences for consideration:  81\n",
            "Current sentence index:  4\n",
            "Longest sentence is:  265\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 80\n",
            "Processing from 80 to 81\n",
            "76\n",
            "Adding sentence:  Їжачок дуже зрадів і погодився .\n",
            "Number of sentences for consideration:  80\n",
            "Current sentence index:  5\n",
            "Longest sentence is:  288\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 80\n",
            "42\n",
            "Adding sentence:  За годину заволочив і ..\n",
            "Number of sentences for consideration:  79\n",
            "Current sentence index:  6\n",
            "Longest sentence is:  280\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 79\n",
            "23\n",
            "Adding sentence:  Ходи сюди , а .\n",
            "Number of sentences for consideration:  78\n",
            "Current sentence index:  7\n",
            "Longest sentence is:  271\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 78\n",
            "25\n",
            "Adding sentence:  Дав мені - Бог пару бичків .\n",
            "Number of sentences for consideration:  77\n",
            "Current sentence index:  8\n",
            "Longest sentence is:  284\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 77\n",
            "19\n",
            "Adding sentence:  І - у ліс .\n",
            "Number of sentences for consideration:  76\n",
            "Current sentence index:  9\n",
            "Longest sentence is:  267\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 76\n",
            "47\n",
            "Adding sentence:  А та гора надi водою .\n",
            "Number of sentences for consideration:  75\n",
            "Current sentence index:  10\n",
            "Longest sentence is:  278\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 75\n",
            "2\n",
            "Adding sentence:  Всі вони обоє і пішли шукати курей .\n",
            "Number of sentences for consideration:  74\n",
            "Current sentence index:  11\n",
            "Longest sentence is:  292\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 74\n",
            "16\n",
            "Adding sentence:  Аж , разу каже той чоловік :\n",
            "Number of sentences for consideration:  73\n",
            "Current sentence index:  12\n",
            "Longest sentence is:  284\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 73\n",
            "0\n",
            "Adding sentence:  Але приходить до тхір до лиса , а лис каже :\n",
            "Number of sentences for consideration:  72\n",
            "Current sentence index:  13\n",
            "Longest sentence is:  300\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 72\n",
            "2\n",
            "Adding sentence:  Каже - до тхора :\n",
            "Number of sentences for consideration:  71\n",
            "Current sentence index:  14\n",
            "Longest sentence is:  273\n",
            "Processing from 0 to 10\n",
            "Processing from 10 to 20\n",
            "Processing from 20 to 30\n",
            "Processing from 30 to 40\n",
            "Processing from 40 to 50\n",
            "Processing from 50 to 60\n",
            "Processing from 60 to 70\n",
            "Processing from 70 to 71\n",
            "50\n",
            "Adding sentence:  Тільки було йому дуже ..\n",
            "Number of sentences for consideration:  70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYK7PomYJbop",
        "colab_type": "code",
        "outputId": "d309fd34-a61e-44c1-b875-1ddf7eb2feb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "story"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Раз прийшов лис до нього в гості , та й тхір його гарно погостив .',\n",
              " 'Тут - курник .',\n",
              " 'Так йому Бог допоміг , що він одної ночі все зробив .',\n",
              " 'Та й ..',\n",
              " 'Ангел - :',\n",
              " 'Їжачок дуже зрадів і погодився .',\n",
              " 'За годину заволочив і ..',\n",
              " 'Ходи сюди , а .',\n",
              " 'Дав мені - Бог пару бичків .',\n",
              " 'І - у ліс .',\n",
              " 'А та гора надi водою .',\n",
              " 'Всі вони обоє і пішли шукати курей .',\n",
              " 'Аж , разу каже той чоловік :',\n",
              " 'Але приходить до тхір до лиса , а лис каже :',\n",
              " 'Каже - до тхора :',\n",
              " 'Тільки було йому дуже ..']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FqON4sOZGnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned = []\n",
        "for j in range(len(story)):\n",
        "      sentence = story[j]\n",
        "      sentence = sentence.replace(\" ?\", \"?\")\n",
        "      sentence = sentence.replace(\" !\", \"!\")\n",
        "      sentence = sentence.replace(\" :\", \":\")\n",
        "      sentence = sentence.replace(\"  \", \"\")\n",
        "      sentence = sentence.replace(\" ,\", \",\")\n",
        "      sentence = sentence.replace(\" .\", \".\")\n",
        "      cleaned.append(sentence)\n",
        "    # cleaned.append(text_to_work)\n",
        "# story = \" \".join(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ4vQlAYZaGL",
        "colab_type": "code",
        "outputId": "0fc3d27a-d56d-4a47-8854-c0b442b9cd4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "story"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Раз прийшов лис до нього в гості , та й тхір його гарно погостив .',\n",
              " 'Тут - курник .',\n",
              " 'Так йому Бог допоміг , що він одної ночі все зробив .',\n",
              " 'Та й ..',\n",
              " 'Ангел - :',\n",
              " 'Їжачок дуже зрадів і погодився .',\n",
              " 'За годину заволочив і ..',\n",
              " 'Ходи сюди , а .',\n",
              " 'Дав мені - Бог пару бичків .',\n",
              " 'І - у ліс .',\n",
              " 'А та гора надi водою .',\n",
              " 'Всі вони обоє і пішли шукати курей .',\n",
              " 'Аж , разу каже той чоловік :',\n",
              " 'Але приходить до тхір до лиса , а лис каже :',\n",
              " 'Каже - до тхора :',\n",
              " 'Тільки було йому дуже ..']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DRkScuprwLn",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Nu7EyMr1t_",
        "colab_type": "text"
      },
      "source": [
        "While this was an interesting experiment, unfortunately, the trained model did a poor of comprising a good story. One of the reasons might be that the input for this step was the output of the trained model from the last step, but some masks were just substituted by punctuation marks and so it was rather hard to infer a context from it. Another reason for this performance might be that the task of new story generation is wrong in its assumption. The assumption was that the stories for kids and fairytales are similar in context and so something new and interesting might be generated from it. As we can see, the model didn't give any meaningful results, so it's possible that when stories are picked in a way that their characters and plots are similar, the model might do a much better job of comprising a new story based on Next Sentence Prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCh1hLkytLNr",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OihblhqPtOlB",
        "colab_type": "text"
      },
      "source": [
        "Ukrainian Stories For Kids Generation Based On Multilingual Bert was definitely an interesting and educating project. Throughout the development the concepts of Transformers and Attention were learned. This blog was particularly useful for visualization and understanding of basic concepts:\n",
        "http://jalammar.github.io.\n",
        "\n",
        "One of the biggest challenges was to formulate the objective and to choose the right model. Ukrainian language was chosen for a reason since a lot less Data Scientists contribute to it and it imposes more obstacles than English or Russian. \n",
        "\n",
        "The dataset of fairytales and children stories was hand-picked, downloaded and saved in order to train the model. This particualr dataset was used because one of the main assumptions was that stories for children have much smaller vocabulary and simpler language constructions, and thus the model after training might yield better results.\n",
        "\n",
        "As we can see from the Part 1, trained model does a better job identifiying masked tokens than the original multilingual BERT. It's been pointed out that original multilingual BERT sometimes outputs words or letters from incorrect language, which proves the relevance of this final project. Part 2 results, however, are not so shiny since the model had a tough job predicting the next sentence. This might be improved by gathering a bigger dataset and creating a bigger corpus for the model to learn from.\n",
        "\n",
        "\n",
        "All in all, this project included a lot of interesting ideas, details, steps and techniques, some of which are:\n",
        "\n",
        "\n",
        "*   Gathering and using completely new dataset and not relying on an existing one\n",
        "*   Using state-of-the art BERT model with transformer and attention mechanisms\n",
        "*   Formulating non-trivial task in terms of NLP\n",
        "*   Choosing Ukrainian as it's harder and imposes more obstacles than English\n",
        "\n",
        "\n",
        "This project required not only knowledge from DataRoot University, but also understanding of frontier ML concepts and thus led to very interesting outcomes.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}